{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate the pipeline\n",
    "from pyannote.audio import Pipeline\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "import os\n",
    "from pydub import AudioSegment\n",
    "from pydub.utils import mediainfo\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token = os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe(audio):\n",
    "    sr, y = audio\n",
    "    \n",
    "    # Convert to mono if stereo\n",
    "    if y.ndim > 1:\n",
    "        y = y.mean(axis=1)\n",
    "        \n",
    "    y = y.astype(np.float32)\n",
    "    y /= np.max(np.abs(y))\n",
    "\n",
    "    return transcriber({\"sampling_rate\": sr, \"raw\": y})[\"text\"]  \n",
    "\n",
    "def convert_mp3_to_wav(mp3_path, wav_path):\n",
    "    audio = AudioSegment.from_mp3(mp3_path)\n",
    "    audio.export(wav_path, format=\"wav\")\n",
    "    return wav_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\anaconda3\\envs\\torchenv\\Lib\\inspect.py:1007: UserWarning: Module 'speechbrain.pretrained' was deprecated, redirecting to 'speechbrain.inference'. Please update your script. This is a change from SpeechBrain 1.0. See: https://github.com/speechbrain/speechbrain/releases/tag/v1.0.0\n",
      "  if ismodule(module) and hasattr(module, '__file__'):\n"
     ]
    }
   ],
   "source": [
    "transcriber = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-base.en\",device=0)\n",
    "diarization_pipeline = Pipeline.from_pretrained(\n",
    "  \"pyannote/speaker-diarization-3.1\",\n",
    "  use_auth_token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"audio.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_converation_from_audio(file_path):\n",
    "    ext = os.path.splitext(file_path)[1]\n",
    "    if ext == \".mp3\":\n",
    "        wav_path = convert_mp3_to_wav(file_path, \"audio.wav\")\n",
    "    else:\n",
    "        wav_path = file_path\n",
    "\n",
    "    diarization = diarization_pipeline(wav_path)\n",
    "    audio_meta = diarization.to_lab()\n",
    "    audio_meta = audio_meta.split('\\n')\n",
    "    conversation_text = []\n",
    "\n",
    "    for meta in audio_meta:\n",
    "        try:\n",
    "            starttime, endtime, speaker = meta.split(' ')\n",
    "            # use whisper to extract text. \n",
    "            audio = AudioSegment.from_file(\"audio.wav\")\n",
    "            extracted_segment = audio[float(starttime)*1000:float(endtime)*1000]\n",
    "            text = transcribe((extracted_segment.frame_rate, np.array(extracted_segment.get_array_of_samples())))\n",
    "            # print(f\"Speaker {speaker} said: {text}\")\n",
    "            conversation_text.append(f\"Speaker {speaker}: {text}\")\n",
    "        except:\n",
    "            pass\n",
    "    return conversation_text\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\anaconda3\\envs\\torchenv\\Lib\\site-packages\\transformers\\models\\whisper\\generation_whisper.py:496: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "conversation_text = get_converation_from_audio(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# openai chat completion\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "system_message = \"\"\"You are a NLP Agent. You analyze conversation between Agent and customer and provide insights on, \n",
    "- Sentiment (Positive, Negative, Neutral)\n",
    "- Intent/Topic List\n",
    "- Entity List\n",
    "- Issue List\n",
    "- Resolution Summary\n",
    "- Customer Satisfaction\n",
    "\"\"\"\n",
    "def get_completion(conversation_text):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": conversation_text\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    return completion.choices[0].message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionMessage(content='Here are the insights based on the conversation:\\n\\n- **Sentiment**: Neutral\\n- **Intent/Topic List**: \\n  - Travel (discussion about travel from Wales to London)\\n  - Family (visiting family, mentioning cousins)\\n  - Local knowledge (questions about London and transportation)\\n- **Entity List**: \\n  - Locations: Wales, London, Oxford, Marble Arch\\n  - Family Members: Cousins, Mum, Dad\\n  - Transportation: Bus, Train\\n- **Issue List**: \\n  - Travel concerns (how long it takes to travel, mode of transportation)\\n- **Resolution Summary**: The conversation revolves around travel from Wales to London, discussing the time it takes and the means of transportation, along with family connections.\\n- **Customer Satisfaction**: Indeterminate from this interaction, as there are no clear indications of satisfaction or dissatisfaction expressed by the customer. \\n\\nThe conversation appears casual and friendly, primarily focused on travel logistics and personal connections.', refusal=None, role='assistant', function_call=None, tool_calls=None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_completion(\"\\n\".join(conversation_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioProcessor:\n",
    "    def __init__(self):\n",
    "        load_dotenv()\n",
    "        self.hf_token = os.getenv(\"HF_TOKEN\")\n",
    "        self.transcriber = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-base.en\", device=0)\n",
    "        self.diarization_pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization-3.1\", use_auth_token=self.hf_token)\n",
    "        self.client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "        self.system_message = \"\"\"You are a NLP Agent. You analyze conversation between Agent and customer and provide insights on, \n",
    "        - Sentiment (Positive, Negative, Neutral)\n",
    "        - Intent/Topic List\n",
    "        - Entity List\n",
    "        - Issue List\n",
    "        - Resolution Summary\n",
    "        - Customer Satisfaction\n",
    "        \"\"\"\n",
    "\n",
    "    def transcribe(self, audio):\n",
    "        sr, y = audio\n",
    "        if y.ndim > 1:\n",
    "            y = y.mean(axis=1)\n",
    "        y = y.astype(np.float32)\n",
    "        y /= np.max(np.abs(y))\n",
    "        return self.transcriber({\"sampling_rate\": sr, \"raw\": y})[\"text\"]\n",
    "\n",
    "    def convert_mp3_to_wav(self, mp3_path, wav_path):\n",
    "        audio = AudioSegment.from_mp3(mp3_path)\n",
    "        audio.export(wav_path, format=\"wav\")\n",
    "        return wav_path\n",
    "\n",
    "    def get_conversation_from_audio(self, file_path):\n",
    "        ext = os.path.splitext(file_path)[1]\n",
    "        if ext == \".mp3\":\n",
    "            wav_file_path = file_path.replace(\".mp3\", \".wav\")\n",
    "            wav_path = self.convert_mp3_to_wav(file_path, wav_file_path)\n",
    "        else:\n",
    "            wav_path = file_path\n",
    "            \n",
    "        print(\"wav path\", wav_path)\n",
    "        diarization = self.diarization_pipeline(wav_path)\n",
    "        audio_meta = diarization.to_lab().split('\\n')\n",
    "        conversation_text = []\n",
    "\n",
    "        for meta in audio_meta:\n",
    "            try:\n",
    "                starttime, endtime, speaker = meta.split(' ')\n",
    "                audio = AudioSegment.from_file(wav_path)\n",
    "                extracted_segment = audio[float(starttime) * 1000:float(endtime) * 1000]\n",
    "                text = self.transcribe((extracted_segment.frame_rate, np.array(extracted_segment.get_array_of_samples())))\n",
    "                conversation_text.append(f\"Speaker {speaker}: {text}\")\n",
    "            except:\n",
    "                pass\n",
    "        return conversation_text\n",
    "\n",
    "    def get_completion(self, conversation_text):\n",
    "        completion = self.client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": self.system_message},\n",
    "                {\"role\": \"user\", \"content\": conversation_text}\n",
    "            ]\n",
    "        )\n",
    "        return completion.choices[0].message\n",
    "\n",
    "processor = AudioProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
